{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e187e518",
   "metadata": {},
   "source": [
    "# <left style=\"font-size:1.0em;\">**About this Code**</left>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align:justify; font-size:1.2em;\">\n",
    "    \n",
    "This code is consisting of architecture of 9 Deep learning and 5 Machine learning algorithms. Models are trained firstly using Radar features in 30 days and 15 days time difference between the TanDEM-X and ICESat-2 data acquisition dates. Further models are trained using both Radar and Environmental features from ECMWF data for 30 days and 15 days time difference. The step of data selection is done in the data preprocessing part. After the training of model, it is applied to the validation data set. History of Deep learning algorithms can also be saved. Further models are evaluated on the test data which is included in the next step of model evaluation and bias estimation.   \n",
    "</div> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74478ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Add, LeakyReLU, BatchNormalization, Conv1D, Flatten, MaxPooling1D, Reshape, MultiHeadAttention, LayerNormalization, Dense, Input, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, precision_score, recall_score, f1_score\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "import json\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fec0bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save models and histories\n",
    "output_dir =\"/folder/Save_results/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to save the model and its training history\n",
    "def save_model_and_history(model, history, model_name):\n",
    "    # Save the model\n",
    "    model_path = os.path.join(output_dir, f\"{model_name}.h5\")\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "    # Save the history\n",
    "    history_path = os.path.join(output_dir, f\"{model_name}_history.json\")\n",
    "    with open(history_path, \"w\") as f:\n",
    "        json.dump(history.history, f)\n",
    "    print(f\"History saved to {history_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1076adb1",
   "metadata": {},
   "source": [
    "### 1-Fully Connected NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fully Connected')\n",
    "model = Sequential()\n",
    "model.add(Input(shape=X_train.shape[1],))\n",
    "model.add(Dense(64, activation='tanh'))\n",
    "model.add(Dense(32, activation='tanh'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(optimizer=Adam(learning_rate=0.0008), loss='mean_squared_error')\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train_np, epochs=100, batch_size=64,  validation_data=(X_val, y_val_np), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad595452",
   "metadata": {},
   "source": [
    "### 2-Batch Normalization NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953510b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fully Batch')\n",
    "modelBatch = Sequential()\n",
    "modelBatch.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "modelBatch.add(BatchNormalization())\n",
    "modelBatch.add(Dense(64, activation='relu'))\n",
    "modelBatch.add(BatchNormalization())\n",
    "modelBatch.add(Dense(1, activation='linear'))\n",
    "modelBatch.compile(optimizer=Adam(learning_rate=0.0005), loss='mean_squared_error')\n",
    "# Train the model\n",
    "historyBatch = modelBatch.fit(X_train, y_train_np, epochs=100, batch_size=64,  validation_data=(X_val, y_val_np), verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdaba5c",
   "metadata": {},
   "source": [
    "### 3-Leaky ReLU NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438c0a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_leaky = Sequential()\n",
    "model_leaky.add(Dense(128, input_dim=X_train.shape[1]))\n",
    "model_leaky.add(LeakyReLU(alpha=0.1))\n",
    "model_leaky.add(BatchNormalization()) \n",
    "model_leaky.add(Dense(64))\n",
    "model_leaky.add(LeakyReLU(alpha=0.1))\n",
    "model_leaky.add(BatchNormalization())\n",
    "model_leaky.add(Dense(1, activation='linear'))\n",
    "model_leaky.compile(optimizer=Adam(learning_rate=0.0005), loss='mean_squared_error')\n",
    "# Train the model\n",
    "historyLeaky = model_leaky.fit(X_train, y_train_np, epochs=100, batch_size=64,  validation_data=(X_val, y_val_np), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389984ef",
   "metadata": {},
   "source": [
    "### 4-Residual NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1e0924",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(X_train.shape[1],))\n",
    "x = Dense(128, activation='relu')(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = BatchNormalization()(x) \n",
    "residual = Dense(128, activation='relu')(input_layer)  # Residual connection\n",
    "x = Add()([x, residual])\n",
    "x = Dense(64, activation='relu')(x)  \n",
    "x = Dense(32, activation='relu')(x)\n",
    "output_layer = Dense(1, activation='linear')(x)\n",
    "model_residual = Model(inputs=input_layer, outputs=output_layer)\n",
    "model_residual.compile(optimizer=Adam(learning_rate=0.0005), loss='mean_squared_error')\n",
    "# Train the model\n",
    "history_residual = model_residual.fit(X_train, y_train_np, epochs=100, batch_size=64,  validation_data=(X_val, y_val_np), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30001b4",
   "metadata": {},
   "source": [
    "### 5-DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd6711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "print('Deep')\n",
    "\n",
    "model_deep = Sequential([\n",
    "    Dense(1024, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),  \n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Dense(512, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.1),\n",
    "    \n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.1),\n",
    "    \n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0005)\n",
    "model_deep.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history_deep = model_deep.fit(\n",
    "    X_train, y_train_np, \n",
    "    epochs=100, \n",
    "    batch_size=256,  \n",
    "    validation_data=(X_val, y_val_np), \n",
    "    callbacks=[early_stopping,lr_scheduler],  \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd285b6a",
   "metadata": {},
   "source": [
    "### 6-Transformer Multi-Head Attention NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ad9b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input\n",
    "input_layer = Input(shape=(X_train.shape[1],))\n",
    "# Add a Dense layer to expand dimensions for attention mechanism\n",
    "x = Dense(64, activation='relu')(input_layer)\n",
    "\n",
    "# Reshape to fit MultiHeadAttention\n",
    "x = tf.expand_dims(x, axis=1)  # Add temporal dimension\n",
    "# MultiHeadAttention layer\n",
    "attention_output = MultiHeadAttention(num_heads=8, key_dim=32)(x, x)\n",
    "# Add normalization\n",
    "x = LayerNormalization(epsilon=1e-6)(attention_output + x)\n",
    "# Flatten and add dense layers\n",
    "ffn = Dense(128, activation='relu')(x)\n",
    "ffn = Dense(64, activation='relu')(ffn)\n",
    "x = LayerNormalization(epsilon=1e-6)(x + ffn) \n",
    "x = tf.squeeze(x, axis=1)  # Remove the temporal dimension\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "output_layer = Dense(1, activation='linear')(x)\n",
    "# Build and compile the model\n",
    "model_attention = Model(inputs=input_layer, outputs=output_layer)\n",
    "model_attention.compile(optimizer=Adam(learning_rate=0.0005), loss='mean_squared_error')\n",
    "# Train the model\n",
    "history_attention = model_attention.fit(X_train, y_train_np, epochs=100, batch_size=64, validation_data=(X_val, y_val_np), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05a2be5",
   "metadata": {},
   "source": [
    "### 7-Auto Encoder NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8207e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the autoencoder\n",
    "input_layer = Input(shape=(X_train.shape[1],))\n",
    "encoded = Dense(64, activation='relu')(input_layer)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(X_train.shape[1], activation='linear')(decoded)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "\n",
    "# Encoder for regression task\n",
    "encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "# Regression model\n",
    "encoded_input = Input(shape=(32,))\n",
    "regression = Dense(32, activation='relu')(encoded_input) \n",
    "\n",
    "regression_output = Dense(1, activation='linear')(regression)\n",
    "regression_model = Model(inputs=encoded_input, outputs=regression_output)\n",
    "# Compile and train the autoencoder\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.008), loss='mse')\n",
    "\n",
    "autoencoder.fit(X_train, X_train, epochs=100, batch_size=64, validation_data=(X_val, X_val), shuffle=True, verbose=1)\n",
    "# Use encoded features for regression\n",
    "X_train_encoded = encoder.predict(X_train)\n",
    "X_test_encoded = encoder.predict(X_test)\n",
    "X_val_encoded = encoder.predict(X_val)\n",
    "regression_model.compile(optimizer=Adam(learning_rate=0.008), loss='mse')\n",
    "#Train the model\n",
    "history_encoder = regression_model.fit(\n",
    "    X_train_encoded, y_train_np, epochs=100, batch_size=64, \n",
    "    validation_data=(X_val_encoded, y_val_np), shuffle=True, verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcba04e9",
   "metadata": {},
   "source": [
    "### 8-MLP NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc61911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP architecture\n",
    "def create_mlp(input_dim, output_dim, task_type=\"regression\"):\n",
    "    \"\"\"\n",
    "    Create a Multi-Layer Perceptron (MLP) model.\n",
    "\n",
    "    Parameters:\n",
    "    - input_dim: Number of input features.\n",
    "    - output_dim: Number of output features (1 for regression or number of classes for classification).\n",
    "    - task_type: \"regression\" or \"classification\".\n",
    "\n",
    "    Returns:\n",
    "    - model: Compiled Keras MLP model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    # Input Layer\n",
    "    model.add(Dense(256, input_dim=input_dim, activation='relu'))\n",
    "  \n",
    "    # Hidden Layers\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "\n",
    "    # Output Layer\n",
    "    if task_type == \"regression\":\n",
    "        model.add(Dense(output_dim, activation='linear'))  # Linear activation for regression\n",
    "        loss_function = 'mean_squared_error'  # Regression loss\n",
    "    elif task_type == \"classification\":\n",
    "        if output_dim == 1:\n",
    "            model.add(Dense(output_dim, activation='sigmoid'))  # Sigmoid for binary classification\n",
    "            loss_function = 'binary_crossentropy'  # Binary classification loss\n",
    "        else:\n",
    "            model.add(Dense(output_dim, activation='softmax'))  # Softmax for multi-class classification\n",
    "            loss_function = 'sparse_categorical_crossentropy'  # Multi-class classification loss\n",
    "    else:\n",
    "        raise ValueError(\"task_type must be 'regression' or 'classification'.\")\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0008),\n",
    "                  loss=loss_function,\n",
    "                  metrics=['accuracy'] if task_type == \"classification\" else ['mae'])\n",
    "\n",
    "    return model\n",
    "\n",
    "input_dim = len(features)  # Number of input features\n",
    "output_dim = 1  # Single output for regression\n",
    "task_type = \"regression\"  # Choose \"regression\" or \"classification\"\n",
    "\n",
    "mlp_model = create_mlp(input_dim, output_dim, task_type)\n",
    "\n",
    "\n",
    "mlp_model.summary()\n",
    "\n",
    "# Train the model\n",
    "\n",
    "mlp_model.fit(X_train, y_train_np, validation_data=(X_val, y_val_np), epochs=50, batch_size=64, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91e3eee",
   "metadata": {},
   "source": [
    "### 9-Wide and Deep NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b63047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import concatenate, Dense, Dropout, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.keras.backend as K\n",
    "import keras_tuner as kt\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    rmse = K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
    "    mean_error = K.mean(y_true - y_pred)\n",
    "    return rmse + 0.1 * K.abs(mean_error)  # Adjust weight for bias correction\n",
    "\n",
    "\n",
    "# Wide Branch\n",
    "input_wide = Input(shape=(X_train.shape[1],))\n",
    "wide_output = Dense(32, activation='relu')(input_wide)\n",
    "\n",
    "# Deep Branch\n",
    "input_deep = Input(shape=(X_train.shape[1],))\n",
    "x = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(input_deep)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = BatchNormalization()(x) \n",
    "x = Dropout(0.2)(x)  \n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = BatchNormalization()(x) \n",
    "x = Dropout(0.2)(x) \n",
    "deep_output = Dense(32, activation='relu')(x)\n",
    "\n",
    "# Combine Wide and Deep\n",
    "merged = concatenate([wide_output, deep_output])\n",
    "output = Dense(1, activation='linear')(merged)\n",
    "\n",
    "model_wide_deep = Model(inputs=[input_wide, input_deep], outputs=output)\n",
    "model_wide_deep.compile(optimizer=Adam(learning_rate=0.0005), loss=combined_loss)\n",
    "\n",
    "# Prepare the data\n",
    "X_train_wide = X_train  \n",
    "X_train_deep = X_train \n",
    "X_test_wide = X_test \n",
    "X_test_deep = X_test\n",
    "X_val_wide = X_val\n",
    "X_val_deep = X_val\n",
    "\n",
    "# Train the model\n",
    "history_wide_deep = model_wide_deep.fit(\n",
    "    [X_train_wide, X_train_deep],  # Two inputs\n",
    "    y_train_np,  # Single target\n",
    "    validation_data=([X_val_wide, X_val_deep],y_val_np),\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "val_loss = model_wide_deep.evaluate([X_val_wide, X_val_deep], y_val_np, verbose=1)\n",
    "print(f\"Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6b2f77",
   "metadata": {},
   "source": [
    "### 10-Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427497bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "#Random Forest algorithm variables are defined\n",
    "rf_model = RandomForestRegressor(n_estimators=150, max_depth=20, random_state=42)\n",
    "#Model Training\n",
    "rf_model.fit(X_train, y_train_np)\n",
    "# Evaluating on the validation set\n",
    "y_val_pred = rf_model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val_np, y_val_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_val_np, y_val_pred))\n",
    "\n",
    "r2 = r2_score(y_val_np, y_val_pred)\n",
    "\n",
    "print(f\"Validation MAE: {mae:.4f}\")\n",
    "print(f\"Validation RMSE: {rmse:.4f}\")\n",
    "print(f\"Validation R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eff3c3",
   "metadata": {},
   "source": [
    "### 11-KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baea20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "#KNN algorithm variables are defined\n",
    "knn = KNeighborsRegressor(n_neighbors=7, metric='manhattan',weights='distance')\n",
    "\n",
    "#Model Training\n",
    "knn.fit(X_train, y_train_np)\n",
    "# Evaluating on the validation set\n",
    "y_pred = knn.predict(X_val)\n",
    "\n",
    "mse = mean_squared_error(y_val_np, y_pred)\n",
    "r2 = r2_score(y_val_np, y_pred)\n",
    "\n",
    "print(\"mse\",mse)\n",
    "print(\"r2 score\",r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7251d5",
   "metadata": {},
   "source": [
    "### 12-Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205bd004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train_np)\n",
    "test_data = lgb.Dataset(X_test, label=y_test_np, reference=train_data)\n",
    "\n",
    "# Light GBM variables are defined\n",
    "params = {\n",
    "    'objective': 'regression',  \n",
    "    'metric': 'mse', \n",
    "    'boosting_type': 'gbdt', \n",
    "    'num_leaves': 64,  \n",
    "    'learning_rate': 0.05,  \n",
    "    'feature_fraction': 0.9,  \n",
    "    'lambda_l2': 0.1 \n",
    "}\n",
    "\n",
    "# Model Training\n",
    "num_round = 500\n",
    "bst = lgb.train(params, train_data, num_round, valid_sets=[test_data])\n",
    "\n",
    "# Evaluating on the validation set\n",
    "y_pred = bst.predict(X_val, num_iteration=bst.best_iteration)\n",
    "\n",
    "\n",
    "mse = mean_squared_error(y_val_np, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "mae = mean_absolute_error(y_val_np, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "\n",
    "r2 = r2_score(y_val_np, y_pred)\n",
    "\n",
    "print(f\"Validation MAE: {mae:.4f}\")\n",
    "print(f\"Validation RMSE: {rmse:.4f}\")\n",
    "print(f\"Validation R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7df41b8",
   "metadata": {},
   "source": [
    "### 13-XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50e0415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train_np)\n",
    "dval = xgb.DMatrix(X_val, label=y_val_np)\n",
    "\n",
    "# XG Boost variables are defined\n",
    "params = {\n",
    "    'objective': 'reg:squarederror', \n",
    "    'eval_metric': 'rmse',  \n",
    "    'learning_rate': 0.05, \n",
    "    'max_depth': 6, \n",
    "    'colsample_bytree': 0.8,  \n",
    "    'subsample': 0.8, \n",
    "    \n",
    "}\n",
    "\n",
    "# Model Training\n",
    "xgb_reg = xgb.train(params, dtrain, num_boost_round=500, evals=[(dval, 'test')], early_stopping_rounds=20, verbose_eval=50)\n",
    "\n",
    "# Evaluating on the validation set\n",
    "y_pred = xgb_reg.predict(dval)\n",
    "\n",
    "mae = mean_absolute_error(y_val_np, y_pred)\n",
    "mse = mean_squared_error(y_val_np, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_val_np, y_pred))\n",
    "r2 = r2_score(y_val_np, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared Score: {r2:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"rmse: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03acaba",
   "metadata": {},
   "source": [
    "### 14-Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c20ff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Decision Tree variables are defined\n",
    "dt_regressor = DecisionTreeRegressor(max_depth=15, min_samples_split=10, min_samples_leaf=5,criterion='squared_error')\n",
    "\n",
    "# Model Training\n",
    "dt_regressor.fit(X_train, y_train_np)\n",
    "\n",
    "# Evaluating on validation set\n",
    "y_pred = dt_regressor.predict(X_val)\n",
    "\n",
    "\n",
    "mse = mean_squared_error(y_val_np, y_pred)\n",
    "r2 = r2_score(y_val_np, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e92b280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save model history\n",
    "\n",
    "save_model_and_history(model, history, \"Fully_Connected\")\n",
    "save_model_and_history(modelBatch, historyBatch, \"Batch_Normalization_NN\")\n",
    "save_model_and_history(model_leaky, historyLeaky, \"Leaky_ReLU_NN\")\n",
    "save_model_and_history(model_residual, history_residual, \"Residual_NN\")\n",
    "save_model_and_history(model_deep, history_deep, \"Deep_NN\")\n",
    "save_model_and_history(model_attention, history_attention , \"Transformer_Multihead_NN\")\n",
    "save_model_and_history(regression_model, history_encoder, \"Autoencoder_Regression_NN\")\n",
    "save_model_and_history(model_wide_deep, history_wide_deep, \"WideAndDeep_NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6065c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function is defined to plot training history\n",
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"\n",
    "    Plots training and validation loss for a given model.\n",
    "    \n",
    "    Parameters:\n",
    "    - history: Keras History object returned by model.fit().\n",
    "    - model_name: Name of the model to display in the plot title.\n",
    "    \"\"\"\n",
    "    save_dir =\"folder/Results/\" # To give directory to save plot history\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'Training and Validation Loss for {model_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.savefig(f\"{save_dir}/{model_name.replace(' ', '_')}_loss.jpeg\", dpi=300)\n",
    "    plt.close()  \n",
    "\n",
    "\n",
    "\n",
    "# Plot each model's training history\n",
    "plot_training_history(history, \"Fully Connected NN\")\n",
    "plot_training_history(historyBatch, \"Batch Normalization NN\")\n",
    "plot_training_history(historyLeaky, \"Leaky ReLU NN\")\n",
    "plot_training_history(history_residual, \"Residual NN\")\n",
    "plot_training_history(history_deep, \"Deep NN\")\n",
    "plot_training_history(mlp_model, \"MLP\")\n",
    "plot_training_history(history_attention, \"Multi-Head Attention NN\")\n",
    "plot_training_history(history_encoder, \"Autoencoder Regression NN\")\n",
    "plot_training_history(history_wide_deep, \"Wide and Deep NN\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
