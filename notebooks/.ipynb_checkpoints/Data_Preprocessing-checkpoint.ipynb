{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a0a3b5a",
   "metadata": {},
   "source": [
    "# <left style=\"font-size:1.0em;\">**About this code**</left>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align:justify; font-size:1.2em;\">\n",
    "    \n",
    "In this study, a large dataset comprising about 300,000 datapoints with TanDEM-X and ICESat-2 elevation measurements, along with ancillary atmospheric and surface variables, is ingested and preprocessed. Firstly, a custom data loading function is implemented to efficiently read the dataset in chunks, minimizing memory overhead, while selectively converting time fields from seconds to days for consistency in temporal analyses. \n",
    "    \n",
    "For prelieminary data analysis a function is made for generating histograms which may be used for analyzing each variable to assess their distributions and identify potential anomalies.\n",
    "    \n",
    "Subsequently, the dataset is filtered to retain only those records where the temporal difference between TanDEM-X and ICESat-2 acquisitions falls within a ±30-day window and ±15-day window for model training and evaluation. A subset of relevant features, encompassing radar-based parameters (e.g., Coherence, Amplitude, Slope, Local Incidence Angle) and environmental variables (e.g., surface temperature, snowfall, wind characteristics), is selected for model training.\n",
    "    \n",
    "The data is partitioned into training, validation, and testing sets using random sampling. Outliers in the training data are identified and removed based on a Z-score thresholding approach to enhance model robustness. Feature standardization is subsequently applied to ensure uniformity in feature scales, thereby facilitating efficient training of the neural network models. The primary objective is to predict the penetration bias between radar and laser altimetry measurements, which is treated as the target variable in the supervised learning framework.\n",
    "</div> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25636f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator)\n",
    "\n",
    "def read_large_csv(file_path, convert_seconds_to_days=True):\n",
    "    \n",
    "    # Define the fields (columns) to be read \n",
    "    columns_to_read = ['lon_(deg)', 'lat_(deg)', 't_dem_(yrs)', 't_dem_-_t_is2_(sec)',\n",
    "       'h_is2_(m)', 'x_3031', 'y_3031', 'temperature_(K)',\n",
    "       'snowfall_(m_in_water_equivalent_)', 'Surface_Pressure_(Pa)',\n",
    "       'Near_Infrared_Albedo', 'Snow_Density(kg/m3)',\n",
    "       'Snow_Depth_(m_in_water_equivalent)',\n",
    "       'Snow_Evaporation_(m_in_water_equivalent)',\n",
    "       'Temperature_of_snow_layer_(K)', 'Total_column_snow_water(kg/m2)',\n",
    "       'Total_precipitation(m)', 'wind_speed_(m/s)',\n",
    "       'wind_direction_(in_degrees)_w.r.t_East', 'Snow_Albedo',\n",
    "       'UV_visible_Albedo', 'TanDEM-X_DEM', 'Slope', 'TanDEM-X_polar',\n",
    "       'Penetration_bias', 'Amplitude', 'Coherence', 'HOA', 'LIA',\n",
    "       'dem_diff_Tdx_Tdxpolar']\n",
    "    \n",
    "    # To read the file in chunks   \n",
    "    chunksize = 10_000  # To adjust based on available memory\n",
    "    df_iterator = pd.read_csv(file_path, usecols=columns_to_read, chunksize=chunksize)\n",
    "    \n",
    "    total_entries = 0\n",
    "    data_chunks = []\n",
    "    for chunk in df_iterator:\n",
    "        total_entries += len(chunk)  #To count the number of rows in each chunk\n",
    "        \n",
    "        #To convert seconds to days if the option is enabled  \n",
    "        if convert_seconds_to_days:\n",
    "            chunk[\"t_dem_-_t_is2_(sec)\"] = chunk[\"t_dem_-_t_is2_(sec)\"] / 86400\n",
    "        \n",
    "        data_chunks.append(chunk)\n",
    "    \n",
    "    print(f\"Total entries in the dataset: {total_entries}\")\n",
    "    \n",
    "    #To combine all chunks into a single DataFrame \n",
    "    data = pd.concat(data_chunks, ignore_index=True)\n",
    "    return data\n",
    "\n",
    "def plot_histograms(data):\n",
    "    for column in data.columns:\n",
    "        print(column)\n",
    "        plt.figure()\n",
    "        plt.hist(data[column].dropna(), bins=20, alpha=0.7, edgecolor='black')  \n",
    "        plt.title(f\"Histogram of {column}\")\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb1578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Input_fil_data.csv\"  # To give the CSV file path\n",
    "df = read_large_csv(file_path)\n",
    " '''Variable of selecting difference of number of days between TanDEM-X and ICESat-2 elevation data.\n",
    " Two values are considered for generating dataset namely 30 days and 15 days'''\n",
    "TIME= 30 # Can be set as 30 or 15\n",
    "filtered_df = df[(df['t_dem_-_t_is2_(sec)'] >= -TIME) & (df['t_dem_-_t_is2_(sec)'] <= TIME)]\n",
    "\n",
    "# Print the number of rows in the filtered DataFrame\n",
    "print(f\"Number of rows in filtered_df: {len(filtered_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac5eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(filtered_df['Penetration_bias'].dropna(), bins=40, alpha=0.7, edgecolor='black',range=[-10, 10]) \n",
    "plt.title(f\"Histogram of Penetration Bias\")\n",
    "plt.xlabel(\"TanDEM_X-IceSAT-2 Elevation(m)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "# Extract the true and predicted values\n",
    "true_values = filtered_df[\"h_is2_(m)\"]\n",
    "predicted_values = filtered_df[\"TanDEM-X_DEM\"]\n",
    "# RMSE\n",
    "rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "# Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(true_values, predicted_values)\n",
    "# Errors (differences between true and predicted values)\n",
    "errors = predicted_values - true_values\n",
    "# Minimum Error\n",
    "min_error = np.min(errors)\n",
    "# Maximum Error\n",
    "max_error = np.max(errors)\n",
    "# Mean Error (Bias)\n",
    "mean_error = np.mean(errors)\n",
    "# Standard Deviation of Errors\n",
    "std_error = np.std(errors)\n",
    "# Print the results\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MSE: {mean_squared_error(true_values, predicted_values):.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"Min Error: {min_error:.4f}\")\n",
    "print(f\"Max Error: {max_error:.4f}\")\n",
    "print(f\"MEDIAN: {filtered_df['Penetration_bias'].median()}\")\n",
    "print(f\"Mean Error (Bias): {mean_error:.4f}\")\n",
    "print(f\"Standard Deviation of Errors: {std_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266d7d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Add, LeakyReLU, BatchNormalization, Conv1D, Flatten, MaxPooling1D, Reshape, MultiHeadAttention, LayerNormalization, Dense, Input, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, precision_score, recall_score, f1_score\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "# Define features and target variable\n",
    "['lon_(deg)', 'lat_(deg)', 't_dem_(yrs)', 't_dem_-_t_is2_(sec)',\n",
    "       'h_is2_(m)', 'x_3031', 'y_3031', 'temperature_(K)',\n",
    "       'snowfall_(m_in_water_equivalent_)', 'Surface_Pressure_(Pa)',\n",
    "       'Near_Infrared_Albedo', 'Snow_Density(kg/m3)',\n",
    "       'Snow_Depth_(m_in_water_equivalent)',\n",
    "       'Snow_Evaporation_(m_in_water_equivalent)',\n",
    "       'Temperature_of_snow_layer_(K)', 'Total_column_snow_water(kg/m2)',\n",
    "       'Total_precipitation(m)', 'wind_speed_(m/s)',\n",
    "       'wind_direction_(in_degrees)_w.r.t_East', 'Snow_Albedo',\n",
    "       'UV_visible_Albedo', 'TanDEM-X_DEM', 'Slope', 'TanDEM-X_polar',\n",
    "       'Penetration_bias', 'Amplitude', 'Coherence', 'HOA', 'LIA',\n",
    "       'dem_diff_Tdx_Tdxpolar']\n",
    "\n",
    "#Both Radar and ECMWF Features\n",
    "features = [\"TanDEM-X_DEM\",\"Coherence\", \"Amplitude\", \"HOA\", \"LIA\",\"Slope\",\"temperature_(K)\", \"snowfall_(m_in_water_equivalent_)\", \"Surface_Pressure_(Pa)\",\"Snow_Density(kg/m3)\",\n",
    "             \"Snow_Evaporation_(m_in_water_equivalent)\", \"wind_speed_(m/s)\", \"wind_direction_(in_degrees)_w.r.t_East\"]\n",
    "\n",
    "#Only Radar features\n",
    "#features = [\"TanDEM-X_DEM\",\"Coherence\", \"Amplitude\", \"HOA\", \"LIA\",\"Slope\"]\n",
    "\n",
    "target = 'Penetration_bias'\n",
    "\n",
    "# Prepare the data\n",
    "X = filtered_df[features]\n",
    "y = filtered_df[target]\n",
    "\n",
    "# Split train & test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=None,shuffle=True\n",
    ")\n",
    "\n",
    "# Split train further into train & validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42,stratify=None,shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets with indices\n",
    "X_train = pd.DataFrame(X_train, index=y_train.index)\n",
    "X_test = pd.DataFrame(X_test, index=y_test.index)\n",
    "\n",
    "\n",
    "train_idx, test_idx = X_train.index, X_test.index\n",
    "\n",
    "\n",
    "filtered_df_y_test = filtered_df.loc[test_idx]\n",
    "\n",
    "\n",
    "#To remove outliers\n",
    "from scipy.stats import zscore\n",
    "import pandas as pd\n",
    "\n",
    "# To compute Z-scores\n",
    "z_scores = X_train.apply(zscore)\n",
    "\n",
    "# Define a threshold (commonly 3 or 2.5)\n",
    "threshold = 2.5\n",
    "mask = (z_scores < threshold).all(axis=1)\n",
    "\n",
    "# Filter out outliers\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# This step is done to convert the y data into numpy array which was earlier in pandas df format\n",
    "y_train_np = y_train.to_numpy().astype(np.float64)\n",
    "y_test_np = y_test.to_numpy().astype(np.float64)\n",
    "y_val_np = y_val.to_numpy().astype(np.float64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
